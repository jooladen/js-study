# import requests
# from bs4 import BeautifulSoup

# # í¬ë¡¤ë§ ëŒ€ìƒ URL (ë„¤ì´ë²„ ë‰´ìŠ¤ ì˜ˆì‹œ)
# url = "https://news.naver.com/"

# # HTML ê°€ì ¸ì˜¤ê¸°
# res = requests.get(url)
# soup = BeautifulSoup(res.text, "html.parser")

# # ì œëª© íƒœê·¸ ì˜ˆì‹œ ì¶”ì¶œ
# titles = soup.select("a[href*='/read?oid=']")[:5]

# for i, t in enumerate(titles, start=1):
#     print(f"{i}. {t.text.strip()}")


# import requests
# from bs4 import BeautifulSoup

# print("ğŸŸ¢ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘!")

# # ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”ì¸ í˜ì´ì§€
# url = "https://news.naver.com/"

# # ìš”ì²­ ë³´ë‚´ê¸° (User-Agent ì¶”ê°€: ì¼ë¶€ ì‚¬ì´íŠ¸ëŠ” í•„ìš”)
# headers = {"User-Agent": "Mozilla/5.0"}
# res = requests.get(url, headers=headers)

# # HTML íŒŒì‹±
# soup = BeautifulSoup(res.text, "html.parser")

# # ë‰´ìŠ¤ ì œëª© ë§í¬ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 5ê°œë§Œ)
# titles = soup.select("a[href*='/read?oid=']")[:5]

# print(f"ğŸ” ì´ {len(titles)}ê°œì˜ ë‰´ìŠ¤ ë°œê²¬")
# for i, t in enumerate(titles, start=1):
#     print(f"{i}. {t.text.strip()}")

# print("âœ… í¬ë¡¤ë§ ì™„ë£Œ")


import requests
from bs4 import BeautifulSoup

print("ğŸŸ¢ ë„¤ì´ë²„ ë‰´ìŠ¤ RSS í¬ë¡¤ë§ ì‹œì‘!")

# ë„¤ì´ë²„ ì£¼ìš” ë‰´ìŠ¤ RSS (ì •ì¹˜ ë‰´ìŠ¤ ì˜ˆì‹œ)
url = "https://rss.naver.com/rss/section_politics.xml"

res = requests.get(url)
soup = BeautifulSoup(res.content, "xml")

# item íƒœê·¸ ì•ˆì˜ title ê°€ì ¸ì˜¤ê¸° (ì•ì—ì„œ 5ê°œë§Œ)
titles = soup.find_all("title")[1:6]  # ì²« ë²ˆì§¸ëŠ” ì±„ë„ ì œëª©ì´ë¼ ì œì™¸

print(f"ğŸ” ì´ {len(titles)}ê°œì˜ ë‰´ìŠ¤ ë°œê²¬")
for i, t in enumerate(titles, start=1):
    print(f"{i}. {t.text.strip()}")

print("âœ… í¬ë¡¤ë§ ì™„ë£Œ")


# import requests
# from bs4 import BeautifulSoup

# url = "http://feeds.bbci.co.uk/news/rss.xml"

# res = requests.get(url)
# soup = BeautifulSoup(res.content, "xml")

# titles = soup.find_all("title")[1:6]

# for i, t in enumerate(titles, 1):
#     print(f"{i}. {t.text.strip()}")
